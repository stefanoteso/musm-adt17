\documentclass[12pt,a4paper]{article}
\usepackage{researchpack}

\usepackage{xcolor}
\newcommand{\andrea}[1]{{\bf \textcolor{blue}{{Andrea: #1}}}}
\newcommand{\stefano}[1]{{\bf \textcolor{violet}{{Stefano: #1}}}}
\newcommand{\bereket}[1]{{\bf \textcolor{red}{{Bereket: #1}}}}

\title{Social Elicitation}

\begin{document}
\maketitle

\paragraph{Setting.} We have:

\begin{itemize}

    \item A set of configurations $\calX$ implicitly defined by hard constraints.

    \item A configuration $\vx \in \calX$ is a vector of $N$ 0-1 attributes.

    \item A set of users $u \in \{1, \ldots, M\}$.

    \item Users have true preferences $\vw^u_* \in \bbR^N_+$.

    \item Users have learned preferences $\vw^u \in \bbR^N_+$.  These may be
        learned using multi-user setmargin.

    \item Let $W \in \bbR^{N \times M}_+$ be the matrix obtained by stacking
        the preference vectors side-by-side. $W_*$ is the analogue for the
        true preferences.

    \item We assume that the weights $W$ are observed.

    \item The aggregate preference of the users \emph{as a group} are
        defined as a function $f$ of $W$.

        Initially, we will focus on linear combinations:
        %
        $$ f(W) := \sum_{u=1}^M \omega_u \vw^u $$

        Linear combinations can capture the contribution of different users
        to the overall utility of configurations.

    \item Our goal is to learn the parameters $\vomega \in \bbR^M$ from
        collective, group-wise feedback.

\end{itemize}

\paragraph{Collective feedback.} The overall idea is:

\begin{itemize}

    \item Feedback is collective: when queried with a set of $S$ configurations
        $\vx^1, \ldots, \vx^S$, the group indicates the overall most preferred
        configuration, say $\vx^i$.

        (Let's just use $S = 2$ for starters.)

    \item We update the user contributions $\vomega$ to $f$ using the
        feedback: users whose $\vw^u$ ranks $\vx^i$ high are most important,
        and users who rank $\vx^i$ low are less important.

    \item Feedback: $\vx^u$ being chosen out of the $S$ query configurations
        amounts to a set of $S - 1$ ranking constraints of the form:
        %
        \begin{align*}
            \forall j \in [S], j \ne i
                & \quad f(\vx^i) \succcurlyeq f(\vx^j)    \\
                & \quad \Longleftrightarrow \langle \sum_{u=1}^M \omega_u \vw^u, \vx^i \rangle \ge \langle \sum_{u=1}^M \omega_u \vw^u, \vx^j \rangle \\
                & \quad \Longleftrightarrow  \sum_{u=1}^M \omega_u \langle \vw^u, \vx^i \rangle \ge \sum_{u=1}^M \omega_u \langle \vw^u, \vx^j \rangle \\
                & \quad \Longleftrightarrow  \sum_{u=1}^M \omega_u \langle \vw^u, \vx^i - \vx^j \rangle \ge 0
        \end{align*}
        %
        The gradient of this expression w.r.t. $\omega_u$ for a single pair
        $i, j$ is:
        %
        $$ \frac{\partial}{\partial \omega_u} \sum_u \omega_u \langle \vw^u, \vx^i - \vx^j \rangle = \langle \vw^u, \vx^i - \vx^j \rangle $$

        This leads to a Perceptron update of the form:
        %
        $$ \omega^{t+1}_u \gets \omega^t_u + \eta \sum_{j \ne i} \langle \vw^u, \vx^i - \vx^j \rangle $$
        %
        or, more coincisely:
        %
        $$ \vomega^{t+1} \gets \vomega^t + \eta W^\top \sum_{j \ne i} (\vx^i - \vx^j) $$
        %
        where $\eta$ is the step size (just $\frac{2}{S(S - 1)}$ for starters).

    \item The (initial) algorithm is as follows:

        \begin{itemize}

            \item Initialize $\vomega^1$ to zero (or at random, e.g. from a
                standard Normal distribution).

            \item Select a set of $S$ objects with maximal utility and
                diversity, for instance by solving:
                %
                \begin{align*}
                    \max_{\vx^1, \ldots, \vx^2}
                        & \;\; \lambda \sum_{i=1}^S f(\vx^i) + (1 - \lambda) \sum_{i < j} \|\vx^i - \vx_j\|_1 \\
                    \text{s.t.}
                        & \;\; f(\vx^i) \ne f(\vx^j)
                            & \forall i,j \in [S], i < j
                \end{align*}
                %
                where $f$ is computed using the current value for $\vomega$.

                This can be done \emph{incrementally}: select $\vx^1$ with
                maximal aggregate utility $f$; then, for all $i = 2, \ldots,
                S$, select $\vx^i$ with maximal aggregate utility $f$ and
                diversity from $\vx^1, \ldots, \vx^{i-1}$. This should be
                much faster.

                \stefano{@Andrea, @Bereket: improvements are welcome.}

            \item Use the Perceptron update above to obtain $\vomega^{t+1}$.

            \item Repeat until convergence or a maximum number of iterations
                $T$ elapsed.

        \end{itemize}

\end{itemize}

\paragraph{Competitors.}

WRITEME

\paragraph{Experiments.}

\begin{itemize}

    \item The group is simulated using a Plackett-Luce user response model,
        see the paper.

        \stefano{This may be sub-optimal for groupwise-feedback; suggestions
        are welcome.}

\end{itemize}

\paragraph{Related work.}

\begin{itemize}

    \item \stefano{@Bereket: please add here the  relevant literature that
        you found, I'll do the same as time permits.}

\end{itemize}

\end{document}
